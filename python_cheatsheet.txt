Pythonチートシート

>  ライブラリ
import pandas as pd
import numpy as np
import os
import glob
import re
import random
from datetime import datetime as dt
import datetime
import time
import json
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
pd.options.display.precision = 2 # pandasの小数点表記
# コマンドライン引数
import sys
import argparse

# 自作ライブラリ
import sys
sys.path.append("lib") # 自作ライブラリのパスを追加
import lib
import lib.preprocess as pr
import importlib
importlib.reload(dr) # ライブラリの中身を更新したらreloadしないと単純に読み込んでも反映されない





>  モジュールのインポートエラー（パス通ってない）
ModuleNotFoundError: No module named 'kfp'
モジュールがインストールされてる場所を確認
!pip show モジュール名
パスを追加
import sys
print(sys.path) # 現在通ってるパスの一覧が表示される。ここにpip showした結果が入ってなければ
sys.path.append('/path/to/hogehoge')






>  Jupyter Notebook
-  進捗（tqdm）
from tqdm import tqdm
for _ in tqdm(リストやrange()などイテラブルオブジェクト):
	...

-  変数の保存
%store               # 変数の一覧表示
%store 変数名     # .ipythonあたりに保存されるらしい
%store -r            # 保存されているすべての変数の読み込み
%store -d 変数名 # 保存した変数を削除
%store -z           # 保存した全ての変数の削除

-  pandasのprintを「...」などで省略しないようにする
print(pd.get_option("display.max_rows")) # df.head()で表示する列数
pd.set_option('display.max_rows', 500) # max_columnsでも同じ


-  stringをn桁になるよう0で埋める
f"{i:04}" # 0001
str(15).zfill(5) # 00015






>  コマンドライン引数
def parse_options(args_str):
    parser = argparse.ArgumentParser(
        prog='xx.py',
        usage='python xx.py [] []\n',
        description='...',
        epilog='end',
        add_help=True
    )
    parser.add_argument('--arg1', type=int, default=0)
    args = parser.parse_args(args_str)
    return args

if __name__ == "__main__":
    args_string = sys.argv[1:]
    print(args_string)
    main(args_string)









>  行列抽出
df.iloc[1]	 	# 1行だけ抽出
df.iloc[1:3] 		# 複数行抽出
df.iloc[1,3] 		# 行数、列数指定して特定のデータを抽出
df.iloc[1:3, 2:5] 	# 行数、列数指定して複数抽出
df["col1"][1:3] 	# 行数、列名指定して特定のデータを抽出

# df[1]、df[1,3]などはダメ
# df[1:3]はできるけど紛らわしいからilocで統一するといい
# df[1, "col1"]はダメ、ilocのとき列は番号のみok

行名でも可能
df['20210102'] # x?
df.loc[['行名']]

df["data"][-1:] # dffの最後の要素を取り出す（df["data"].tail(1)でもおｋ）




>  列の順序入れ替え、並べ替え
df = df.reindex(columns=[cols]) # 任意の順番に
df = df.sort_index() # ソート



>  pandas一般
-  DataFrame -> ndarray
df.values # でok。()などいらない

-  マルチインデックス、マルチカラム
df.columns = [df.columns, 追加インデックスのlist]
# マルチインデックスで並び替え
df = df.sort_index(level=1, axis=1) # level1(level0の下)で並び替え. 列ごと並び替えの時はsort_indexにaxis=1指定


-  各列に対して一括で操作: apply
df.apply(type) # 各列のmax # min, typeなども可能
df.nunique() 	# ユニーク数
df.apply(float.is_integer).any() # [True, False, ...]の列に一つでもTrueがあればTrue
.all() # [True, False, ...]の列が全てTrueならTrue


-  pandas 重複削除(抽出)
drop_duplicates(subset="列名", keep="first")


-  値が切り替わっている箇所のみ抽出
.ne()でnot equalかどうかをboolで返す、shift()で一つ次のdf
df["col"].ne(df["col"].shift())





>  plot
plt.add_subplotは追加、plt.subplotは上書き、plt.subplotsはfigとax同時定義

-   DataFrameの特定の列だけ重ねて簡易的にplot
df[['sepal_length', 'sepal_width']].plot()

-  オプション
color="tab:green" # デフォルトの色
linestyle='dashed' # solid, dashed, dashdot, dotted
linewidth=3
- marker
https://pythondatascience.plavox.info/matplotlib/%E3%83%9E%E3%83%BC%E3%82%AB%E3%83%BC%E3%81%AE%E5%90%8D%E5%89%8D
- 色
https://pythondatascience.plavox.info/matplotlib/%E8%89%B2%E3%81%AE%E5%90%8D%E5%89%8D

# 余白
plt.subplots_adjust(hspace=0.4)


-  軸
# 図の軸の数値を指数表記から普通の表記に変換
plt.ticklabel_format(style='plain',axis='y') # style="plain"が普通、"sci"が指数表記
# 横軸
ax1.plot(横軸にしたいリストやSeries, df["data"])で横軸も設定可能。時刻とかもおｋ


-  横に大きいグラフ（1つ）
fig, ax = plt.subplots(figsize=(10, 5), dpi=200)
ax.plot(x1)
ax.set_title("")


-  横に大きいグラフ（2つ並べる）
fig = plt.figure(figsize=(10, 10), dpi=200)
ax1 = fig.add_subplot(2, 1, 1)
ax1.plot(df["col1"], label="", color="tab:blue")
ax1.plot(df["col2"], label="", color="tab:orange")
ax1.legend()
ax1.set_xlabel("")
ax1.set_ylabel("")
ax1.set_title("")
# ax1.set_xlim(0, 1)
# ax1.set_ylim(0, 1)

ax2 = fig.add_subplot(2, 1, 2)
ax2.plot(df["col3"], label="", color="tab:blue")
ax2.plot(df["col4"], label="", color="tab:orange")
ax2.legend()
ax2.set_xlabel("")
ax2.set_ylabel("")
ax2.set_title("")
# ax2.set_xlim(0, 1)
# ax2.set_ylim(0, 1)

fig.savefig("img_{}.png".format(dt_now))




-  二軸
fig = plt.figure(figsize=(10, 10), dpi=200)
ax1 = fig.add_subplot(2, 1, 1)
ax2 = ax1.twinx()
ax1.plot(df["col1"], label="", color="tab:blue")
ax2.plot(df["col2"], label="", color="tab:orange"
h1, l1 = ax1.get_legend_handles_labels()
h2, l2 = ax2.get_legend_handles_labels()
ax1.legend(h1+h2, l1+l2, loc='lower right')
ax1.set_title("")
ax1.set_xlabel("")
ax1.set_ylabel("")
ax2.set_ylabel("")







>  ヒストグラム
# weights = np.ones_like(df["col1"]) / len(df["col1"]) # 相対度数
fig = plt.figure(figsize=(10, 10), dpi=200)
ax1 = fig.add_subplot(2, 1, 1)
ax1.hist(df["col1"], bins=50, label="", color="tab:blue") # , weights=weights)
ax1.set_title("histogram")
ax1.set_xlabel("data")
ax1.set_ylabel("frequency")
ax1.legend()



-  他のplot
ax1.vlines(data, ymin, ymax, color="tab:green", label="") # たて線
ax1.hlines(data, xmin, xmax, color="tab:green", label="") # よこ線











>  型変換
bool_list.astype(int) # boolean → int（plotしたいときなど）


>  forループ, 内包表記
-  逆から
for i in reverse(list):
-  一行ずつ
for index, row in df.iterrows():

-  二重の内包表記
[(o + "_" + i) for o in outer for i in inner] # 通常の二重ループと書く順番は一緒
odd_list = [x for x in r if x % 2] # forが先

-  forループで二つの変数を扱う
[k + "_" + str(v) for (k, v) in zip(js.keys(), js.values())]






>  リスト

-  シーケンス生成
list(range(0, len(df))) # range()だけじゃタプルなので？ダメ。インデックス生成などに使える
range(0, 10, 1) 	# range(0, 10) = 0～9
np.arange(0, 10) 	# array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])


-  最後の要素を取り出す
arr[-1] # （dfではdf["data"][-1]は×、df["data"][-1:]とかで〇）

-  逆さまに並べ替え
list.reverse()	 	# 元リスト自体書き換えられる、list2 = list.reverse()は×
list2 = list[::-1] 	# 元リストは保持

-  リスト同士の結合（新たに作るのではなく元リストを加工）
list1.extend(["D", "E"])
list1.append(["D", "E"])
# a = ["A", "B", "C"].extend(["D", "E"]) としても a はNone










>  ファイル操作
os.getcwd() # 現在のフルパス取得
glob.glob(os.path.join(os.getcwd(), "*.csv")) # globはワイルドカードでファイル名取得したいとき
fnames = os.listdir(path) # ワイルドカード使えないので非推奨
fnames.sort() # TODO: 0埋めしてないとき正しく数字で並べかえ

-  ディレクトリ作成
os.makedirs(path, exist_ok=True)
# exist_ok=Trueで、既に存在しているディレクトリを指定してもエラーにならない
# os.mkdir()と違ってos.makedirs()は再帰的に作れる

-  削除
os.remove('test.txt') # ファイルを削除
shutil.rmtree('data') # 空じゃないディレクトリを再帰的に削除
shutil.move("元", "先")
shutil.copy("元", "先")

-  存在確認
os.path.exists(path)
os.path.isfile(path)
os.path.isdir(path)



>  ファイル読み書き

-  csvに値を一行ずつ追加していく(pandasなし)
with open('sample.csv', 'a') as f:
    writer = csv.writer(f)
    writer.writerow(['X', 'Y', 'Z'])

-  読み
with open(path) as f:
	content = f.read()

-  書き
with open(path, 'w') as f:
	f.write(s + "\n")
	# f.write('\n'.join(list)) # リストを一行ずつ
	
# 末尾に追記
with open(path, 'a') as f:
	f.write(s + "\n")



>  JSON
-  JSONファイルを辞書として読み込む
with open(path) as f:
	js_dict = json.load(f) # load
-  JSON文字列 → 辞書
js_dict = json.loads(js_str) # load「s」(strのs)
-  辞書 → JSON文字列
js_str = json.dumps(js_dict) # diump「s」(strのs)
-  辞書をJSONファイルとして保存
with open('path', 'w') as f:
    json.dump(js_dict, f)

-  バイナリが入っているJSON（？）（「b'{...}」というデータ）'
js = json.loads(data.decode("utf-8"))
modeの末尾にbをつける？
要確認



>  大きいデータを途中で保存する
np.save("path", df)
df = np.load("path", allow_pickle=True)


>  辞書
-  キーをリストから登録、値の初期値あり
d = dict.fromkeys([key1, key2], 0)
-  キーと値のリストから登録
d = dict(zip(keys, values))
-  キーの値を取得しつつ、存在しない場合はデフォルト値設定
d = d.get("key", デフォルト値)




>  乱数
-  int
random.randrange(start, stop, step) 	# start <= n < stop からint（stop含まない）
random.randint(start, stop)	 		# start <= n <= stop (stop含む)
-  float
random.random() 					# 0.0以上1.0未満のfloat（一様分布）
random.uniform(start, stop) 			# start <= n <= stop のfloat（一様分布）







>  時系列

datetime型
time型？
timestamp型
date型？
pandasのtimestamp型？datetime型？

-  文字列 ⇄ datetime
# 文字列（ひとつのデータ） → datetime64[ns]
ti = dt.strptime("202103290020", "%Y%m%d%H%M")
# 文字列（列ごと） → datetime64[ns]
ti = pd.to_datetime(df["time"], format="%Y%m%d%H%M")
# datetime → 文字列
ti = dt_now.strftime('%Y%m%d_%H%M%S.%f')
ミリ秒は「%f」

# 日付抽出
pd.to_datetime(df["time"], format="%Y%m%d%H%M").dt.date
.dt.year, .dt.minute, .dt.secondなども可能
.month()など()はいらない
列全体に対しても有効

# 日で丸める（"H"で時間で丸める、"T"で分で丸める、"15T"で15分ごとに丸める）
pd.to_datetime(df["time"], format="%Y%m%d%H%M").dt.round("D")

-  int ⇄ datetime
from datetime import datetime, timedelta
datetime(1899, 12, 30) + timedelta(days=date_int)
https://qiita.com/nezumi/items/23c301c661f5e9653f19


datetime→タイムスタンプ
ts = datetime.datetime.timestamp(dt)


-  足し算引き算
datetime.timedelta(days=1)  # 1日分として足し算に使える
datetime.timedelta(hours=1) # 1時間分

-  現在時刻を文字列で取得
dt_now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

-  時系列インデックス生成
pd.date_range(文字列, periods=50000, freq='40ms') # 5万行ぶん、40msずつ

-  特定の時刻より後を抽出、とか
df[df["time"] > datetime.datetime(2021, 8, 1)]
df[df["time"] > datetime.datetime(2021, 8, 1)]
df[df["time"].dt.date == datetime.date(2021, 5, 1)] # datetime.datetimeではなくdatetime.dateなところに注意

-  フォーマット
https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases
https://docs.python.org/ja/3/library/time.html#time.strftime
B: buiness day frequency
C: ustom business day frequency
D: alendar day frequency
W: eekly frequency
M: onth end frequency
SM: emi-month end frequency (15th and end of month)
BM: usiness month end frequency
CBM: custom business month end frequency
MS: month start frequency
SMS: semi-month start frequency (1st and 15th)
BMS: business month start frequency
CBMS: custom business month start frequency
Q: quarter end frequency
BQ: business quarter end frequency
QS: quarter start frequency
BQS: business quarter start frequency
A, Y: year end frequency
BA, BY: business year end frequency
AS, YS: year start frequency
BAS, BYS: business year start frequency
BH: business hour frequency
H: hourly frequency
T, min: minutely frequency
S: secondly frequency
L, ms: milliseconds
U, us: microseconds
N: nanoseconds


>  pip
pip list        # インストール済パッケージ一覧
pip list -o    # アップデートが必要なパッケージ一覧（-uなら最新のみ表示）
pip install -U パッケージ名  # アップデート
pip freeze > requirements.txt # requirements.txt生成＆更新




>  subprocess
# 標準出力をファイルに保存
with open('out.txt', 'w') as f:
	cp = subprocess.run(['ls', '-1'], stdout=f)


>  pickle
# pandasでおｋ
df = pd.read_pickle('ファイル名.pickle')

import pickle
with open("パス", mode="rb") as f:
    df = pickle.load(f)




>  時間幅展開を戻す
tr = []
for i in range(len(train_picked)):
    tr.extend(list(train_picked.iloc[i, :].values))


>  Awesome

-  条件抽出
df.query(複数条件)
-  サンプリング
df.sample(frac=0.5) # 50%
df.sample(n=100) # 100件

np.where
sort_values
shift
lambda
pd.pivot_table
SatandardScalerで正規化実施オブジェクト生成、オブジェクト.fit_traindform(データ)で平均0,分散1に正規化
MaxMinScalerで最大1, 最小0に正規化
astype(int)



